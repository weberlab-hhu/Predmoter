{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3788e0c",
   "metadata": {},
   "source": [
    "# Testdata creation\n",
    "## Table of Contents\n",
    "1. Create artificial data functions\n",
    "2. Create testdata   \n",
    "    > 2.1 Training data    \n",
    "    > 2.2 Validation data    \n",
    "    > 2.3 Prediction data    \n",
    "    > 2.4 Extra files    \n",
    "4. Look into one file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90597634",
   "metadata": {},
   "source": [
    "## 1. Create artificial data functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55e87d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7bae56c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# seed to generate the same test data as was generated here\n",
    "def set_seed(seed):\n",
    "    os.environ[\"PL_GLOBAL_SEED\"] = str(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a49b4c4c",
   "metadata": {},
   "source": [
    "The function ``create_fake_fasta`` is used to generate a fake fasta file. The genome sequence is randomly generated. The input parameters are ``filename``, the basename for the fasta file, and ``chromosome_map``, a dictionary containing the chromosome information. An example for ``chromosome_map`` would be:    \n",
    "```python\n",
    "{\"Chrom1\": {\"length\": 51321, \"Ns\": [5, 34]},\n",
    " \"Chrom2\": {\"length\": 102642, \"Ns\": [21640, 43402]}}\n",
    "```\n",
    "The dictionary defines the name of each chromosome, i.e., Chrom1 and Chrom2. For each chromosome the total chromosome length and the locations of N bases are defined. The N bases are either defined by a list of start and end position of the N bases/stretch or by setting it to ``None``, which would mean that this chromosome doesn't contain N bases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2385a924",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_fake_fasta(filename, chromosome_map):\n",
    "    bases = np.array([\"A\", \"T\", \"G\", \"C\"])\n",
    "    chromosomes = []\n",
    "    i = 1\n",
    "    for chrom, map_ in chromosome_map.items():\n",
    "        chromosomes.append(f\">{chrom} fake chromosome {i}\")\n",
    "        c = np.random.choice(bases, size=map_[\"length\"], replace=True)\n",
    "        if map_[\"Ns\"] is not None:\n",
    "            c[map_[\"Ns\"][0]:map_[\"Ns\"][1]] = \"N\"\n",
    "        c = np.split(c, np.arange(80, len(c), 80))  # lines of a fasta file are always 80 characters long\n",
    "        c = [\"\".join(list(arr)) for arr in c]\n",
    "        chromosomes.append(\"\\n\".join(c))\n",
    "        i += 1\n",
    "    \n",
    "    with open(f\"{filename}.fa\", \"w\") as f:\n",
    "        f.write(\"\\n\".join(chromosomes))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf850090",
   "metadata": {},
   "source": [
    "The function ``create_testdata`` is used to generate fake h5 files for testing. The test files use the same format as the h5 files created from fasta files with Helixer. The input parameters ``filename`` and ``chromosome_map`` are the same as for ``create_fake_fasta``. The ``species`` parameter can be any fake species name. The ``subsequence_length`` can be any length, the default is 21384 bp, Helixer's standard ``subsequence_length``. An example for ``dataset_info`` would be:    \n",
    "```python\n",
    "{\"atacseq\": 2, \"h3k4me3\": 4}\n",
    "```\n",
    "The dictionary defines the prefix of each dataset and the amount of bam files per dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e30a346c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_testdata(filename, dataset_info, species, chromosome_map, subsequence_length=21384):\n",
    "    h5df = h5py.File(f\"{filename}.h5\", \"w\")\n",
    "    h5df.create_group(\"data\")\n",
    "    h5df.create_group(\"evaluation\")\n",
    "    \n",
    "    # generate X/DNA sequence\n",
    "    total_chunks = 0\n",
    "    X = []\n",
    "    seqids = []\n",
    "    start_ends = []\n",
    "    dsets = [[] for _ in range(len(dataset_info))]  # empty list for each dataset\n",
    "    for chrom, map_ in chromosome_map.items():\n",
    "        chunks = math.ceil(map_[\"length\"]/subsequence_length)\n",
    "        total_chunks += chunks*2\n",
    "        \n",
    "        # Create DNA sequence (data/X)\n",
    "        # -----------------------------\n",
    "        # flat array at first\n",
    "        x = np.eye(4)[np.random.choice(4, (chunks * subsequence_length))]\n",
    "        # generate fake padding (is 'added' when the chromosome is not exactly divisible by the sequence length)\n",
    "        x[map_[\"length\"]:(chunks * subsequence_length)] = [0., 0., 0., 0.]\n",
    "        # add Ns\n",
    "        if map_[\"Ns\"] is not None:\n",
    "            x[map_[\"Ns\"][0]:map_[\"Ns\"][1]] = [0.25, 0.25, 0.25, 0.25]\n",
    "        \n",
    "        # if the chromosome is exactly divisible, flipping is not needed\n",
    "        need_to_flip = map_[\"length\"] % subsequence_length != 0\n",
    "\n",
    "        x = np.concatenate([x, np.flip(x)])  # reverse strand: np.flip(x)\n",
    "        x = np.reshape(x, (chunks*2, subsequence_length, 4))\n",
    "\n",
    "        # flip according to Helixer convention (padded bases are always at the end)\n",
    "        if need_to_flip:\n",
    "            padding_end = np.where(np.sum(x[chunks], axis=1) == 0)[0][-1] + 1\n",
    "            x[chunks] = np.concatenate([x[chunks][padding_end:], x[chunks][:padding_end]])\n",
    "        X.append(x)\n",
    "        \n",
    "        # Create seqids\n",
    "        # --------------\n",
    "        seqids.append(np.full(shape=(chunks*2,), fill_value=chrom, dtype=\"S50\"))\n",
    "        \n",
    "        # Create start_ends\n",
    "        # ------------------\n",
    "        se = np.array([[i, i + subsequence_length] for i in range(0, map_[\"length\"], subsequence_length)])\n",
    "        se[-1][1] = map_[\"length\"]\n",
    "        start_ends.append(np.concatenate([se, np.flip(se)]))  # reverse strand: np.flip(se)\n",
    "        \n",
    "        # Create datasets (exaluation/<dataset>_coverage)\n",
    "        # -------------------------------------------------\n",
    "        for i, dset in enumerate(dataset_info):\n",
    "            # dataset_info[dset] amount of bam files\n",
    "            fake_data = np.random.randint(low=0, high=450, size=(chunks * subsequence_length, dataset_info[dset]))\n",
    "            fake_data[map_[\"length\"]:(chunks * subsequence_length)] = -1\n",
    "            fake_data = np.concatenate([fake_data, np.flip(fake_data)])\n",
    "            fake_data = np.reshape(fake_data, (chunks*2, subsequence_length, dataset_info[dset]))\n",
    "\n",
    "            if need_to_flip:\n",
    "                fake_data[chunks] = np.concatenate([fake_data[chunks][padding_end:], fake_data[chunks][:padding_end]])\n",
    "            dsets[i].append(fake_data)\n",
    "    \n",
    "    # Create h5 file\n",
    "    # ----------------\n",
    "    # data\n",
    "    h5df.create_dataset(\"data/X\", shape=(total_chunks, subsequence_length, 4), maxshape=(None, subsequence_length, None),\n",
    "                        dtype=\"<f2\", compression=\"gzip\", compression_opts=9, data=np.concatenate(X))\n",
    "    h5df.create_dataset(\"data/seqids\", shape=(total_chunks,), maxshape=(None,), dtype=\"S50\", compression=\"gzip\",\n",
    "                        compression_opts=9, data=np.concatenate(seqids))\n",
    "    h5df.create_dataset(\"data/species\", shape=(total_chunks,), maxshape=(None,), dtype=\"S50\", compression=\"gzip\",\n",
    "                        compression_opts=9, data=np.full(shape=(total_chunks,), fill_value=species, dtype=\"S50\"))\n",
    "    h5df.create_dataset(\"data/start_ends\", shape=(total_chunks, 2), maxshape=(None, 2), dtype=\"int\", compression=\"gzip\",\n",
    "                        compression_opts=9, data=np.concatenate(start_ends))\n",
    "    \n",
    "    # evaluation\n",
    "    for i, dset in enumerate(dataset_info):\n",
    "        h5df.create_dataset(f\"evaluation/{dset}_coverage\", shape=(total_chunks, subsequence_length, dataset_info[dset]),\n",
    "                            maxshape=(None, subsequence_length, None), dtype=\"int\", compression=\"lzf\",\n",
    "                            data=np.concatenate(dsets[i]))\n",
    "        bam_files = [f\"fake_{i}.bam\" for j in range(dataset_info[dset])]\n",
    "        h5df.create_dataset(f\"evaluation/{dset}_meta/bam_files\", shape=(dataset_info[dset], ), maxshape=(None, ),\n",
    "                            dtype=\"S512\", data=np.array(bam_files, dtype=\"S512\"))\n",
    "        \n",
    "    h5df.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "314dea62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_blacklist(file, sequence_ids):\n",
    "    sequence_ids = np.array([id_.encode() for id_ in sequence_ids])\n",
    "    h5df = h5py.File(file, \"a\")\n",
    "    chunks = h5df[\"data/X\"].shape[0]\n",
    "    # True: chunk will be retained, False: chunk is masked/blacklisted\n",
    "    mask = ~np.in1d(h5df[\"data/seqids\"], sequence_ids)\n",
    "    h5df.create_dataset(\"data/blacklist\", shape=(chunks,), maxshape=(None,), dtype=np.int32,\n",
    "                        compression=\"gzip\", compression_opts=9, data=mask)\n",
    "    h5df.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afbe6c56",
   "metadata": {},
   "source": [
    "## 2. Create testdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc65b64f",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(998)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50944027",
   "metadata": {},
   "source": [
    "### 2.1 Training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "da7db32a",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_testdata(\"train_data_1\",\n",
    "                {\"atacseq\": 2, \"h3k4me3\": 4}, \"Species_artificialis_1\",\n",
    "                {\"Chrom1\": {\"length\": 88045, \"Ns\": [666, 890]},\n",
    "                 \"Chrom2\": {\"length\": 22447, \"Ns\": None},\n",
    "                 \"Chrom3\": {\"length\": 34566, \"Ns\": None}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2f5ff245",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_testdata(\"train_data_2\",\n",
    "                {\"atacseq\": 1, \"h3k4me3\": 2, \"moaseq\": 2}, \"Species_artificialis_2\",\n",
    "                {\"Chrom1\": {\"length\": 78412, \"Ns\": [42755, 64160]},\n",
    "                 \"Chrom2\": {\"length\": 32122, \"Ns\": None},\n",
    "                 \"Unplaced1\": {\"length\": 20489, \"Ns\": None},\n",
    "                 \"Unplaced2\": {\"length\": 18504, \"Ns\": [91, 107]}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "56b7a164",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_testdata(\"train_data_3\",\n",
    "                {\"moaseq\": 1}, \"Species_artificialis_3\",\n",
    "                {\"Chrom1\": {\"length\": 42768, \"Ns\": None},\n",
    "                 \"Chrom2\": {\"length\": 21384, \"Ns\": None}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bcc996c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_testdata(\"train_data_4\",\n",
    "                {\"atacseq\": 3, \"h3k4me3\": 1}, \"Species_artificialis_4\",\n",
    "                {\"Chrom1\": {\"length\": 58633, \"Ns\": None},\n",
    "                 \"Chrom2\": {\"length\": 19870, \"Ns\": None},\n",
    "                 \"Unplaced1\": {\"length\": 1223, \"Ns\": None},\n",
    "                 \"Unplaced2\": {\"length\": 4587, \"Ns\": None},\n",
    "                 \"Unplaced3\": {\"length\": 12356, \"Ns\": None}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "30fa0393",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_blacklist(\"train_data_4.h5\", [\"Unplaced1\", \"Unplaced2\", \"Unplaced3\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "beed9f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# special: unplaced scaffold in between chromosomes\n",
    "create_testdata(\"train_data_5\",\n",
    "                {\"atacseq\": 2}, \"Species_artificialis_5\",\n",
    "                {\"Chrom1\": {\"length\": 104748, \"Ns\": [20962, 43006]},\n",
    "                 \"Unplaced0_chrom1\": {\"length\": 7655, \"Ns\": [1222, 1299]},\n",
    "                 \"Chrom2\": {\"length\": 40976, \"Ns\": [21, 34]},\n",
    "                 \"Unplaced1\": {\"length\": 6982, \"Ns\": None},\n",
    "                 \"Plastid1\": {\"length\": 9557, \"Ns\": None}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7253f864",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_blacklist(\"train_data_5.h5\", [\"Unplaced0_chrom1\", \"Unplaced1\", \"Plastid1\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba82e3f",
   "metadata": {},
   "source": [
    "### 2.2 Validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "20ee6acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_testdata(\"val_data_1\",\n",
    "                {\"atacseq\": 1, \"h3k4me3\": 2}, \"Species_artificialis_6\",\n",
    "                {\"Chrom1\": {\"length\": 64152, \"Ns\": None},\n",
    "                 \"Chrom2\": {\"length\": 21384, \"Ns\": None}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "529e7c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_testdata(\"val_data_2\",\n",
    "                {\"atacseq\": 1, \"h3k4me3\": 1, \"moaseq\": 1}, \"Species_artificialis_7\",\n",
    "                {\"Chrom1\": {\"length\": 20588, \"Ns\": None},\n",
    "                 \"Chrom2\": {\"length\": 17978, \"Ns\": None}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ff2fa5b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_testdata(\"val_data_3\",\n",
    "                {\"atacseq\": 2}, \"Species_artificialis_8\",\n",
    "                {\"Chrom1\": {\"length\": 85245, \"Ns\": [42600, 64231]}})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bfafca6",
   "metadata": {},
   "source": [
    "### 2.3 Prediction data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "25e35b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_fake_fasta(\"pred_data\",\n",
    "                  {\"Chrom1\": {\"length\": 51321, \"Ns\": [5, 34]},\n",
    "                   \"Chrom2\": {\"length\": 102642, \"Ns\": [21640, 43402]}})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "359b7778",
   "metadata": {},
   "source": [
    "The file ``pred_data.fa`` was converted to two h5 files using Helixer's ``fasta2h5.py``. The two h5 files are ``pred_data_1.h5`` with a subsequence length of 21384 bp, and ``pred_data_2.h5`` with a subsequence length of 42768 bp. The chosen species was ``Species_artificialis_9``."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c865399e",
   "metadata": {},
   "source": [
    "### 2.4 Extra files\n",
    "Additional test data created was the ``mini_model.ckpt``, a very bare bones model checkpoint that was created using all the training and validation data with this command:\n",
    "```bash\n",
    "Predmoter.py -m train -i data -o out --prefix mini -e 5 -b 10 --hidden-size 16 --filter-size 16 \\\n",
    "--seed 5 --num-devices 1 --num-workers 4\n",
    "# all other parameters were the default parameters\n",
    "```\n",
    "The last/fifth model checkpoint was chosen as the ``mini_model.ckpt``. This checkpoint was created with Predmoter version 0.3.2 commit 3ddadf9.   \n",
    "The one prediction file ``pred_data_1_predictions.h5`` was also created with the same Predmoter version and the command:\n",
    "```bash\n",
    "Predmoter.py -m predict -f pred_data_1.h5 --prefix mini -b 10 --model mini_model.ckpt\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fddbc42",
   "metadata": {},
   "source": [
    "## 4. Look into one file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6457b04f",
   "metadata": {},
   "source": [
    "A closer look into the file ``train_data_4.h5`` is shown here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "15027ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "h5df = h5py.File(\"train_data_4.h5\", \"r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "61d5c627",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14, 21384, 4)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h5df[\"data/X\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "969aedb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[    0, 21384],\n",
       "       [21384, 42768],\n",
       "       [42768, 58633],\n",
       "       [58633, 42768],\n",
       "       [42768, 21384],\n",
       "       [21384,     0],\n",
       "       [    0, 19870],\n",
       "       [19870,     0],\n",
       "       [    0,  1223],\n",
       "       [ 1223,     0],\n",
       "       [    0,  4587],\n",
       "       [ 4587,     0],\n",
       "       [    0, 12356],\n",
       "       [12356,     0]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h5df[\"data/start_ends\"][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cf008f89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([b'Chrom1', b'Chrom1', b'Chrom1', b'Chrom1', b'Chrom1', b'Chrom1',\n",
       "       b'Chrom2', b'Chrom2', b'Unplaced1', b'Unplaced1', b'Unplaced2',\n",
       "       b'Unplaced2', b'Unplaced3', b'Unplaced3'], dtype='|S50')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h5df[\"data/seqids\"][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7b517d5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h5df[\"data/blacklist\"][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d7136b4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 1., 0.],\n",
       "       [0., 0., 0., 1.],\n",
       "       [0., 1., 0., 0.],\n",
       "       [0., 1., 0., 0.],\n",
       "       [0., 1., 0., 0.],\n",
       "       [0., 0., 0., 1.],\n",
       "       [0., 0., 1., 0.],\n",
       "       [0., 1., 0., 0.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [0., 0., 1., 0.]], dtype=float16)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h5df[\"data/X\"][0][:10]  # the first 50 bp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "63fcc7f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KeysViewHDF5 ['atacseq_coverage', 'atacseq_meta', 'h3k4me3_coverage', 'h3k4me3_meta']>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h5df[\"evaluation\"].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2d12d852",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[436, 281, 425],\n",
       "       [106, 409, 296],\n",
       "       [256, 189, 223],\n",
       "       [227, 316,  50],\n",
       "       [449, 157,  69],\n",
       "       [106, 335, 364],\n",
       "       [393, 393, 120],\n",
       "       [293, 413, 335],\n",
       "       [ 91, 406,  46],\n",
       "       [371, 122,  39]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h5df[\"evaluation/atacseq_coverage\"][0][:10]  # the artificial ATAC-seq coverage of the first 50 bp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "521a0603",
   "metadata": {},
   "outputs": [],
   "source": [
    "h5df.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e52f68",
   "metadata": {},
   "source": [
    "A loointo the blacklist array of ``train_data_5.h5``:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f7c57821",
   "metadata": {},
   "outputs": [],
   "source": [
    "h5df = h5py.File(\"train_data_5.h5\", \"r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c9293510",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h5df[\"data/blacklist\"][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "82fd3460",
   "metadata": {},
   "outputs": [],
   "source": [
    "h5df.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece8d314",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
